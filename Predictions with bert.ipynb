{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Predictions with bert","version":"0.3.2","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"0jeNg-87Z5G7","colab_type":"code","outputId":"ac150c81-976f-48b4-b569-4d107595c2bb","executionInfo":{"status":"ok","timestamp":1546082316141,"user_tz":-330,"elapsed":46152,"user":{"displayName":"Deep Learning","photoUrl":"","userId":"02647502250279678328"}},"colab":{"base_uri":"https://localhost:8080/","height":120}},"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"metadata":{"id":"WuxHgPYnpQYp","colab_type":"code","outputId":"4f5249f6-43a5-4c4a-f5b7-43926b09259d","executionInfo":{"status":"ok","timestamp":1546082324577,"user_tz":-330,"elapsed":1499,"user":{"displayName":"Deep Learning","photoUrl":"","userId":"02647502250279678328"}},"colab":{"base_uri":"https://localhost:8080/","height":33}},"cell_type":"code","source":["cd drive/My Drive/"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive\n"],"name":"stdout"}]},{"metadata":{"id":"Pj2BIXcUpItE","colab_type":"code","outputId":"a5e50b25-3d14-462e-c847-84b834262776","executionInfo":{"status":"ok","timestamp":1546091837616,"user_tz":-330,"elapsed":3736,"user":{"displayName":"Deep Learning","photoUrl":"","userId":"02647502250279678328"}},"colab":{"base_uri":"https://localhost:8080/","height":134}},"cell_type":"code","source":["!ls"],"execution_count":0,"outputs":[{"output_type":"stream","text":[" clouderizer\t\t\t   pytorch_model.bin\n","'Colab Notebooks'\t\t   pytorch_modeltrained_2_epochs.bin\n"," dev-v1.1.json\t\t\t   testpytorch_model.bin\n"," nbest_predictions_2_epochs.json   testpytorch_modeltrained.bin\n"," nbest_predictions.json\t\t   TrainedModel\n"," predictions_2_epochs.json\t   train-v1.1.json\n"," predictions.json\t\t   train-v1.1.json_bert-base-uncased_128_128\n"],"name":"stdout"}]},{"metadata":{"id":"q0gpk9H_-nie","colab_type":"text"},"cell_type":"markdown","source":["In this Directory You need to have the following\n","1.Prediction Data set\n","2.A folder(/TrainedModel) containing Fine tuned model(pytorch_model.bin) and configuration file(bert_config.json) inside "]},{"metadata":{"id":"bKojqlNAouoe","colab_type":"code","outputId":"3767ac32-e62a-4234-f66f-e950ec7b6e4c","executionInfo":{"status":"ok","timestamp":1546082371335,"user_tz":-330,"elapsed":36762,"user":{"displayName":"Deep Learning","photoUrl":"","userId":"02647502250279678328"}},"colab":{"base_uri":"https://localhost:8080/","height":53}},"cell_type":"code","source":["# http://pytorch.org/\n","from os.path import exists\n","from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n","platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n","cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n","accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n","\n","!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision\n","import torch"],"execution_count":0,"outputs":[{"output_type":"stream","text":["tcmalloc: large alloc 1073750016 bytes == 0x5760e000 @  0x7f009e76e2a4 0x591a07 0x5b5d56 0x502e9a 0x506859 0x502209 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x507641 0x502209 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x507641 0x504c28 0x502540 0x502f3d 0x507641\n"],"name":"stdout"}]},{"metadata":{"id":"igwzk_fAeSMv","colab_type":"code","outputId":"56f493b4-9c9f-4445-bc68-26a3af1e3cbc","executionInfo":{"status":"ok","timestamp":1546082377966,"user_tz":-330,"elapsed":36816,"user":{"displayName":"Deep Learning","photoUrl":"","userId":"02647502250279678328"}},"colab":{"base_uri":"https://localhost:8080/","height":371}},"cell_type":"code","source":["!pip install pytorch-pretrained-bert"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Collecting pytorch-pretrained-bert\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/95/68/84de54aea460eb5b2e90bf47a429aacc1ce97ff052ec40874ea38ae2331d/pytorch_pretrained_bert-0.4.0-py3-none-any.whl (45kB)\n","\r\u001b[K    22% |███████▎                        | 10kB 15.4MB/s eta 0:00:01\r\u001b[K    45% |██████████████▌                 | 20kB 1.5MB/s eta 0:00:01\r\u001b[K    68% |█████████████████████▊          | 30kB 2.3MB/s eta 0:00:01\r\u001b[K    90% |█████████████████████████████   | 40kB 1.6MB/s eta 0:00:01\r\u001b[K    100% |████████████████████████████████| 51kB 1.9MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (2.18.4)\n","Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.9.67)\n","Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (0.4.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.14.6)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (4.28.1)\n","Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (2.6)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (3.0.4)\n","Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (1.22)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (2018.11.29)\n","Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (0.9.3)\n","Requirement already satisfied: s3transfer<0.2.0,>=0.1.10 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (0.1.13)\n","Requirement already satisfied: botocore<1.13.0,>=1.12.67 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (1.12.67)\n","Requirement already satisfied: docutils>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.67->boto3->pytorch-pretrained-bert) (0.14)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.67->boto3->pytorch-pretrained-bert) (2.5.3)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\"->botocore<1.13.0,>=1.12.67->boto3->pytorch-pretrained-bert) (1.11.0)\n","Installing collected packages: pytorch-pretrained-bert\n","Successfully installed pytorch-pretrained-bert-0.4.0\n"],"name":"stdout"}]},{"metadata":{"id":"O9JCPTZio5JI","colab_type":"code","colab":{}},"cell_type":"code","source":["from __future__ import absolute_import\n","from __future__ import division\n","from __future__ import print_function\n","\n","import argparse\n","import collections\n","import logging\n","import json\n","import math\n","import os\n","import random\n","import pickle\n","from tqdm import tqdm, trange\n","\n","import numpy as np\n","import torch\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","from torch.utils.data.distributed import DistributedSampler\n","\n","from pytorch_pretrained_bert.tokenization import whitespace_tokenize, BasicTokenizer, BertTokenizer\n","from pytorch_pretrained_bert.modeling import BertForQuestionAnswering\n","from pytorch_pretrained_bert.optimization import BertAdam\n","from pytorch_pretrained_bert.file_utils import PYTORCH_PRETRAINED_BERT_CACHE"],"execution_count":0,"outputs":[]},{"metadata":{"id":"nXW-SNxGaWWd","colab_type":"code","colab":{}},"cell_type":"code","source":["logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n","                    datefmt = '%m/%d/%Y %H:%M:%S',\n","                    level = logging.INFO)\n","logger = logging.getLogger(__name__)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"AHoM8SGJH0vV","colab_type":"code","colab":{}},"cell_type":"code","source":["class SquadExample(object):\n","    \"\"\"A single training/test example for the Squad dataset.\"\"\"\n","\n","    def __init__(self,\n","                 qas_id,\n","                 question_text,\n","                 doc_tokens,\n","                 orig_answer_text=None,\n","                 start_position=None,\n","                 end_position=None):\n","        self.qas_id = qas_id\n","        self.question_text = question_text\n","        self.doc_tokens = doc_tokens\n","        self.orig_answer_text = orig_answer_text\n","        self.start_position = start_position\n","        self.end_position = end_position\n","\n","    def __str__(self):\n","        return self.__repr__()\n","\n","    def __repr__(self):\n","        s = \"\"\n","        s += \"qas_id: %s\" % (self.qas_id)\n","        s += \", question_text: %s\" % (\n","            self.question_text)\n","        s += \", doc_tokens: [%s]\" % (\" \".join(self.doc_tokens))\n","        if self.start_position:\n","            s += \", start_position: %d\" % (self.start_position)\n","        if self.start_position:\n","            s += \", end_position: %d\" % (self.end_position)\n","        return s\n","\n","\n","class InputFeatures(object):\n","    \"\"\"A single set of features of data.\"\"\"\n","\n","    def __init__(self,\n","                 unique_id,\n","                 example_index,\n","                 doc_span_index,\n","                 tokens,\n","                 token_to_orig_map,\n","                 token_is_max_context,\n","                 input_ids,\n","                 input_mask,\n","                 segment_ids,\n","                 start_position=None,\n","                 end_position=None):\n","        self.unique_id = unique_id\n","        self.example_index = example_index\n","        self.doc_span_index = doc_span_index\n","        self.tokens = tokens\n","        self.token_to_orig_map = token_to_orig_map\n","        self.token_is_max_context = token_is_max_context\n","        self.input_ids = input_ids\n","        self.input_mask = input_mask\n","        self.segment_ids = segment_ids\n","        self.start_position = start_position\n","        self.end_position = end_position\n","\n","\n","def read_squad_examples(input_file, is_training):\n","    \"\"\"Read a SQuAD json file into a list of SquadExample.\"\"\"\n","    with open(input_file, \"r\", encoding='utf-8') as reader:\n","        input_data = json.load(reader)[\"data\"]\n","\n","    def is_whitespace(c):\n","        if c == \" \" or c == \"\\t\" or c == \"\\r\" or c == \"\\n\" or ord(c) == 0x202F:\n","            return True\n","        return False\n","\n","    examples = []\n","    for entry in input_data:\n","        for paragraph in entry[\"paragraphs\"]:\n","            paragraph_text = paragraph[\"context\"]\n","            doc_tokens = []\n","            char_to_word_offset = []\n","            prev_is_whitespace = True\n","            for c in paragraph_text:\n","                if is_whitespace(c):\n","                    prev_is_whitespace = True\n","                else:\n","                    if prev_is_whitespace:\n","                        doc_tokens.append(c)\n","                    else:\n","                        doc_tokens[-1] += c\n","                    prev_is_whitespace = False\n","                char_to_word_offset.append(len(doc_tokens) - 1)\n","\n","            for qa in paragraph[\"qas\"]:\n","                qas_id = qa[\"id\"]\n","                question_text = qa[\"question\"]\n","                start_position = None\n","                end_position = None\n","                orig_answer_text = None\n","                if is_training:\n","                    if len(qa[\"answers\"]) != 1:\n","                        raise ValueError(\n","                            \"For training, each question should have exactly 1 answer.\")\n","                    answer = qa[\"answers\"][0]\n","                    orig_answer_text = answer[\"text\"]\n","                    answer_offset = answer[\"answer_start\"]\n","                    answer_length = len(orig_answer_text)\n","                    start_position = char_to_word_offset[answer_offset]\n","                    end_position = char_to_word_offset[answer_offset + answer_length - 1]\n","                    # Only add answers where the text can be exactly recovered from the\n","                    # document. If this CAN'T happen it's likely due to weird Unicode\n","                    # stuff so we will just skip the example.\n","                    #\n","                    # Note that this means for training mode, every example is NOT\n","                    # guaranteed to be preserved.\n","                    actual_text = \" \".join(doc_tokens[start_position:(end_position + 1)])\n","                    cleaned_answer_text = \" \".join(\n","                        whitespace_tokenize(orig_answer_text))\n","                    if actual_text.find(cleaned_answer_text) == -1:\n","                        logger.warning(\"Could not find answer: '%s' vs. '%s'\",\n","                                           actual_text, cleaned_answer_text)\n","                        continue\n","\n","                example = SquadExample(\n","                    qas_id=qas_id,\n","                    question_text=question_text,\n","                    doc_tokens=doc_tokens,\n","                    orig_answer_text=orig_answer_text,\n","                    start_position=start_position,\n","                    end_position=end_position)\n","                examples.append(example)\n","    return examples\n","\n","\n","def convert_examples_to_features(examples, tokenizer, max_seq_length,\n","                                 doc_stride, max_query_length, is_training):\n","    \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n","\n","    unique_id = 1000000000\n","\n","    features = []\n","    for (example_index, example) in enumerate(examples):\n","        query_tokens = tokenizer.tokenize(example.question_text)\n","\n","        if len(query_tokens) > max_query_length:\n","            query_tokens = query_tokens[0:max_query_length]\n","\n","        tok_to_orig_index = []\n","        orig_to_tok_index = []\n","        all_doc_tokens = []\n","        for (i, token) in enumerate(example.doc_tokens):\n","            orig_to_tok_index.append(len(all_doc_tokens))\n","            sub_tokens = tokenizer.tokenize(token)\n","            for sub_token in sub_tokens:\n","                tok_to_orig_index.append(i)\n","                all_doc_tokens.append(sub_token)\n","\n","        tok_start_position = None\n","        tok_end_position = None\n","        if is_training:\n","            tok_start_position = orig_to_tok_index[example.start_position]\n","            if example.end_position < len(example.doc_tokens) - 1:\n","                tok_end_position = orig_to_tok_index[example.end_position + 1] - 1\n","            else:\n","                tok_end_position = len(all_doc_tokens) - 1\n","            (tok_start_position, tok_end_position) = _improve_answer_span(\n","                all_doc_tokens, tok_start_position, tok_end_position, tokenizer,\n","                example.orig_answer_text)\n","\n","        # The -3 accounts for [CLS], [SEP] and [SEP]\n","        max_tokens_for_doc = max_seq_length - len(query_tokens) - 3\n","\n","        # We can have documents that are longer than the maximum sequence length.\n","        # To deal with this we do a sliding window approach, where we take chunks\n","        # of the up to our max length with a stride of `doc_stride`.\n","        _DocSpan = collections.namedtuple(  # pylint: disable=invalid-name\n","            \"DocSpan\", [\"start\", \"length\"])\n","        doc_spans = []\n","        start_offset = 0\n","        while start_offset < len(all_doc_tokens):\n","            length = len(all_doc_tokens) - start_offset\n","            if length > max_tokens_for_doc:\n","                length = max_tokens_for_doc\n","            doc_spans.append(_DocSpan(start=start_offset, length=length))\n","            if start_offset + length == len(all_doc_tokens):\n","                break\n","            start_offset += min(length, doc_stride)\n","\n","        for (doc_span_index, doc_span) in enumerate(doc_spans):\n","            tokens = []\n","            token_to_orig_map = {}\n","            token_is_max_context = {}\n","            segment_ids = []\n","            tokens.append(\"[CLS]\")\n","            segment_ids.append(0)\n","            for token in query_tokens:\n","                tokens.append(token)\n","                segment_ids.append(0)\n","            tokens.append(\"[SEP]\")\n","            segment_ids.append(0)\n","\n","            for i in range(doc_span.length):\n","                split_token_index = doc_span.start + i\n","                token_to_orig_map[len(tokens)] = tok_to_orig_index[split_token_index]\n","\n","                is_max_context = _check_is_max_context(doc_spans, doc_span_index,\n","                                                       split_token_index)\n","                token_is_max_context[len(tokens)] = is_max_context\n","                tokens.append(all_doc_tokens[split_token_index])\n","                segment_ids.append(1)\n","            tokens.append(\"[SEP]\")\n","            segment_ids.append(1)\n","\n","            input_ids = tokenizer.convert_tokens_to_ids(tokens)\n","\n","            # The mask has 1 for real tokens and 0 for padding tokens. Only real\n","            # tokens are attended to.\n","            input_mask = [1] * len(input_ids)\n","\n","            # Zero-pad up to the sequence length.\n","            while len(input_ids) < max_seq_length:\n","                input_ids.append(0)\n","                input_mask.append(0)\n","                segment_ids.append(0)\n","\n","            assert len(input_ids) == max_seq_length\n","            assert len(input_mask) == max_seq_length\n","            assert len(segment_ids) == max_seq_length\n","\n","            start_position = None\n","            end_position = None\n","            if is_training:\n","                # For training, if our document chunk does not contain an annotation\n","                # we throw it out, since there is nothing to predict.\n","                doc_start = doc_span.start\n","                doc_end = doc_span.start + doc_span.length - 1\n","                if (example.start_position < doc_start or\n","                        example.end_position < doc_start or\n","                        example.start_position > doc_end or example.end_position > doc_end):\n","                    continue\n","\n","                doc_offset = len(query_tokens) + 2\n","                start_position = tok_start_position - doc_start + doc_offset\n","                end_position = tok_end_position - doc_start + doc_offset\n","\n","            if example_index < 20:\n","                logger.info(\"*** Example ***\")\n","                logger.info(\"unique_id: %s\" % (unique_id))\n","                logger.info(\"example_index: %s\" % (example_index))\n","                logger.info(\"doc_span_index: %s\" % (doc_span_index))\n","                logger.info(\"tokens: %s\" % \" \".join(tokens))\n","                logger.info(\"token_to_orig_map: %s\" % \" \".join([\n","                    \"%d:%d\" % (x, y) for (x, y) in token_to_orig_map.items()]))\n","                logger.info(\"token_is_max_context: %s\" % \" \".join([\n","                    \"%d:%s\" % (x, y) for (x, y) in token_is_max_context.items()\n","                ]))\n","                logger.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n","                logger.info(\n","                    \"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n","                logger.info(\n","                    \"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids]))\n","                if is_training:\n","                    answer_text = \" \".join(tokens[start_position:(end_position + 1)])\n","                    logger.info(\"start_position: %d\" % (start_position))\n","                    logger.info(\"end_position: %d\" % (end_position))\n","                    logger.info(\n","                        \"answer: %s\" % (answer_text))\n","\n","            features.append(\n","                InputFeatures(\n","                    unique_id=unique_id,\n","                    example_index=example_index,\n","                    doc_span_index=doc_span_index,\n","                    tokens=tokens,\n","                    token_to_orig_map=token_to_orig_map,\n","                    token_is_max_context=token_is_max_context,\n","                    input_ids=input_ids,\n","                    input_mask=input_mask,\n","                    segment_ids=segment_ids,\n","                    start_position=start_position,\n","                    end_position=end_position))\n","            unique_id += 1\n","\n","    return features\n","\n","\n","def _improve_answer_span(doc_tokens, input_start, input_end, tokenizer,\n","                         orig_answer_text):\n","    \"\"\"Returns tokenized answer spans that better match the annotated answer.\"\"\"\n","\n","    # The SQuAD annotations are character based. We first project them to\n","    # whitespace-tokenized words. But then after WordPiece tokenization, we can\n","    # often find a \"better match\". For example:\n","    #\n","    #   Question: What year was John Smith born?\n","    #   Context: The leader was John Smith (1895-1943).\n","    #   Answer: 1895\n","    #\n","    # The original whitespace-tokenized answer will be \"(1895-1943).\". However\n","    # after tokenization, our tokens will be \"( 1895 - 1943 ) .\". So we can match\n","    # the exact answer, 1895.\n","    #\n","    # However, this is not always possible. Consider the following:\n","    #\n","    #   Question: What country is the top exporter of electornics?\n","    #   Context: The Japanese electronics industry is the lagest in the world.\n","    #   Answer: Japan\n","    #\n","    # In this case, the annotator chose \"Japan\" as a character sub-span of\n","    # the word \"Japanese\". Since our WordPiece tokenizer does not split\n","    # \"Japanese\", we just use \"Japanese\" as the annotation. This is fairly rare\n","    # in SQuAD, but does happen.\n","    tok_answer_text = \" \".join(tokenizer.tokenize(orig_answer_text))\n","\n","    for new_start in range(input_start, input_end + 1):\n","        for new_end in range(input_end, new_start - 1, -1):\n","            text_span = \" \".join(doc_tokens[new_start:(new_end + 1)])\n","            if text_span == tok_answer_text:\n","                return (new_start, new_end)\n","\n","    return (input_start, input_end)\n","\n","\n","def _check_is_max_context(doc_spans, cur_span_index, position):\n","    \"\"\"Check if this is the 'max context' doc span for the token.\"\"\"\n","\n","    # Because of the sliding window approach taken to scoring documents, a single\n","    # token can appear in multiple documents. E.g.\n","    #  Doc: the man went to the store and bought a gallon of milk\n","    #  Span A: the man went to the\n","    #  Span B: to the store and bought\n","    #  Span C: and bought a gallon of\n","    #  ...\n","    #\n","    # Now the word 'bought' will have two scores from spans B and C. We only\n","    # want to consider the score with \"maximum context\", which we define as\n","    # the *minimum* of its left and right context (the *sum* of left and\n","    # right context will always be the same, of course).\n","    #\n","    # In the example the maximum context for 'bought' would be span C since\n","    # it has 1 left context and 3 right context, while span B has 4 left context\n","    # and 0 right context.\n","    best_score = None\n","    best_span_index = None\n","    for (span_index, doc_span) in enumerate(doc_spans):\n","        end = doc_span.start + doc_span.length - 1\n","        if position < doc_span.start:\n","            continue\n","        if position > end:\n","            continue\n","        num_left_context = position - doc_span.start\n","        num_right_context = end - position\n","        score = min(num_left_context, num_right_context) + 0.01 * doc_span.length\n","        if best_score is None or score > best_score:\n","            best_score = score\n","            best_span_index = span_index\n","\n","    return cur_span_index == best_span_index\n","\n","\n","\n","RawResult = collections.namedtuple(\"RawResult\",\n","                                   [\"unique_id\", \"start_logits\", \"end_logits\"])\n","\n","\n","def write_predictions(all_examples, all_features, all_results, n_best_size,\n","                      max_answer_length, do_lower_case, output_prediction_file,\n","                      output_nbest_file, verbose_logging):\n","    \"\"\"Write final predictions to the json file.\"\"\"\n","    logger.info(\"Writing predictions to: %s\" % (output_prediction_file))\n","    logger.info(\"Writing nbest to: %s\" % (output_nbest_file))\n","\n","    example_index_to_features = collections.defaultdict(list)\n","    for feature in all_features:\n","        example_index_to_features[feature.example_index].append(feature)\n","\n","    unique_id_to_result = {}\n","    for result in all_results:\n","        unique_id_to_result[result.unique_id] = result\n","\n","    _PrelimPrediction = collections.namedtuple(  # pylint: disable=invalid-name\n","        \"PrelimPrediction\",\n","        [\"feature_index\", \"start_index\", \"end_index\", \"start_logit\", \"end_logit\"])\n","\n","    all_predictions = collections.OrderedDict()\n","    all_nbest_json = collections.OrderedDict()\n","    for (example_index, example) in enumerate(all_examples):\n","        features = example_index_to_features[example_index]\n","\n","        prelim_predictions = []\n","        for (feature_index, feature) in enumerate(features):\n","            result = unique_id_to_result[feature.unique_id]\n","\n","            start_indexes = _get_best_indexes(result.start_logits, n_best_size)\n","            end_indexes = _get_best_indexes(result.end_logits, n_best_size)\n","            for start_index in start_indexes:\n","                for end_index in end_indexes:\n","                    # We could hypothetically create invalid predictions, e.g., predict\n","                    # that the start of the span is in the question. We throw out all\n","                    # invalid predictions.\n","                    if start_index >= len(feature.tokens):\n","                        continue\n","                    if end_index >= len(feature.tokens):\n","                        continue\n","                    if start_index not in feature.token_to_orig_map:\n","                        continue\n","                    if end_index not in feature.token_to_orig_map:\n","                        continue\n","                    if not feature.token_is_max_context.get(start_index, False):\n","                        continue\n","                    if end_index < start_index:\n","                        continue\n","                    length = end_index - start_index + 1\n","                    if length > max_answer_length:\n","                        continue\n","                    prelim_predictions.append(\n","                        _PrelimPrediction(\n","                            feature_index=feature_index,\n","                            start_index=start_index,\n","                            end_index=end_index,\n","                            start_logit=result.start_logits[start_index],\n","                            end_logit=result.end_logits[end_index]))\n","\n","        prelim_predictions = sorted(\n","            prelim_predictions,\n","            key=lambda x: (x.start_logit + x.end_logit),\n","            reverse=True)\n","\n","        _NbestPrediction = collections.namedtuple(  # pylint: disable=invalid-name\n","            \"NbestPrediction\", [\"text\", \"start_logit\", \"end_logit\"])\n","\n","        seen_predictions = {}\n","        nbest = []\n","        for pred in prelim_predictions:\n","            if len(nbest) >= n_best_size:\n","                break\n","            feature = features[pred.feature_index]\n","\n","            tok_tokens = feature.tokens[pred.start_index:(pred.end_index + 1)]\n","            orig_doc_start = feature.token_to_orig_map[pred.start_index]\n","            orig_doc_end = feature.token_to_orig_map[pred.end_index]\n","            orig_tokens = example.doc_tokens[orig_doc_start:(orig_doc_end + 1)]\n","            tok_text = \" \".join(tok_tokens)\n","\n","            # De-tokenize WordPieces that have been split off.\n","            tok_text = tok_text.replace(\" ##\", \"\")\n","            tok_text = tok_text.replace(\"##\", \"\")\n","\n","            # Clean whitespace\n","            tok_text = tok_text.strip()\n","            tok_text = \" \".join(tok_text.split())\n","            orig_text = \" \".join(orig_tokens)\n","\n","            final_text = get_final_text(tok_text, orig_text, do_lower_case, verbose_logging)\n","            if final_text in seen_predictions:\n","                continue\n","\n","            seen_predictions[final_text] = True\n","            nbest.append(\n","                _NbestPrediction(\n","                    text=final_text,\n","                    start_logit=pred.start_logit,\n","                    end_logit=pred.end_logit))\n","\n","        # In very rare edge cases we could have no valid predictions. So we\n","        # just create a nonce prediction in this case to avoid failure.\n","        if not nbest:\n","            nbest.append(\n","                _NbestPrediction(text=\"empty\", start_logit=0.0, end_logit=0.0))\n","\n","        assert len(nbest) >= 1\n","\n","        total_scores = []\n","        for entry in nbest:\n","            total_scores.append(entry.start_logit + entry.end_logit)\n","\n","        probs = _compute_softmax(total_scores)\n","\n","        nbest_json = []\n","        for (i, entry) in enumerate(nbest):\n","            output = collections.OrderedDict()\n","            output[\"text\"] = entry.text\n","            output[\"probability\"] = probs[i]\n","            output[\"start_logit\"] = entry.start_logit\n","            output[\"end_logit\"] = entry.end_logit\n","            nbest_json.append(output)\n","\n","        assert len(nbest_json) >= 1\n","\n","        all_predictions[example.qas_id] = nbest_json[0][\"text\"]\n","        all_nbest_json[example.qas_id] = nbest_json\n","\n","    with open(output_prediction_file, \"w\") as writer:\n","        writer.write(json.dumps(all_predictions, indent=4) + \"\\n\")\n","\n","    with open(output_nbest_file, \"w\") as writer:\n","        writer.write(json.dumps(all_nbest_json, indent=4) + \"\\n\")\n","\n","\n","def get_final_text(pred_text, orig_text, do_lower_case, verbose_logging=False):\n","    \"\"\"Project the tokenized prediction back to the original text.\"\"\"\n","\n","    # When we created the data, we kept track of the alignment between original\n","    # (whitespace tokenized) tokens and our WordPiece tokenized tokens. So\n","    # now `orig_text` contains the span of our original text corresponding to the\n","    # span that we predicted.\n","    #\n","    # However, `orig_text` may contain extra characters that we don't want in\n","    # our prediction.\n","    #\n","    # For example, let's say:\n","    #   pred_text = steve smith\n","    #   orig_text = Steve Smith's\n","    #\n","    # We don't want to return `orig_text` because it contains the extra \"'s\".\n","    #\n","    # We don't want to return `pred_text` because it's already been normalized\n","    # (the SQuAD eval script also does punctuation stripping/lower casing but\n","    # our tokenizer does additional normalization like stripping accent\n","    # characters).\n","    #\n","    # What we really want to return is \"Steve Smith\".\n","    #\n","    # Therefore, we have to apply a semi-complicated alignment heruistic between\n","    # `pred_text` and `orig_text` to get a character-to-charcter alignment. This\n","    # can fail in certain cases in which case we just return `orig_text`.\n","\n","    def _strip_spaces(text):\n","        ns_chars = []\n","        ns_to_s_map = collections.OrderedDict()\n","        for (i, c) in enumerate(text):\n","            if c == \" \":\n","                continue\n","            ns_to_s_map[len(ns_chars)] = i\n","            ns_chars.append(c)\n","        ns_text = \"\".join(ns_chars)\n","        return (ns_text, ns_to_s_map)\n","\n","    # We first tokenize `orig_text`, strip whitespace from the result\n","    # and `pred_text`, and check if they are the same length. If they are\n","    # NOT the same length, the heuristic has failed. If they are the same\n","    # length, we assume the characters are one-to-one aligned.\n","    tokenizer = BasicTokenizer(do_lower_case=do_lower_case)\n","\n","    tok_text = \" \".join(tokenizer.tokenize(orig_text))\n","\n","    start_position = tok_text.find(pred_text)\n","    if start_position == -1:\n","        if verbose_logging:\n","            logger.info(\n","                \"Unable to find text: '%s' in '%s'\" % (pred_text, orig_text))\n","        return orig_text\n","    end_position = start_position + len(pred_text) - 1\n","\n","    (orig_ns_text, orig_ns_to_s_map) = _strip_spaces(orig_text)\n","    (tok_ns_text, tok_ns_to_s_map) = _strip_spaces(tok_text)\n","\n","    if len(orig_ns_text) != len(tok_ns_text):\n","        if verbose_logging:\n","            logger.info(\"Length not equal after stripping spaces: '%s' vs '%s'\",\n","                            orig_ns_text, tok_ns_text)\n","        return orig_text\n","\n","    # We then project the characters in `pred_text` back to `orig_text` using\n","    # the character-to-character alignment.\n","    tok_s_to_ns_map = {}\n","    for (i, tok_index) in tok_ns_to_s_map.items():\n","        tok_s_to_ns_map[tok_index] = i\n","\n","    orig_start_position = None\n","    if start_position in tok_s_to_ns_map:\n","        ns_start_position = tok_s_to_ns_map[start_position]\n","        if ns_start_position in orig_ns_to_s_map:\n","            orig_start_position = orig_ns_to_s_map[ns_start_position]\n","\n","    if orig_start_position is None:\n","        if verbose_logging:\n","            logger.info(\"Couldn't map start position\")\n","        return orig_text\n","\n","    orig_end_position = None\n","    if end_position in tok_s_to_ns_map:\n","        ns_end_position = tok_s_to_ns_map[end_position]\n","        if ns_end_position in orig_ns_to_s_map:\n","            orig_end_position = orig_ns_to_s_map[ns_end_position]\n","\n","    if orig_end_position is None:\n","        if verbose_logging:\n","            logger.info(\"Couldn't map end position\")\n","        return orig_text\n","\n","    output_text = orig_text[orig_start_position:(orig_end_position + 1)]\n","    return output_text\n","\n","\n","def _get_best_indexes(logits, n_best_size):\n","    \"\"\"Get the n-best logits from a list.\"\"\"\n","    index_and_score = sorted(enumerate(logits), key=lambda x: x[1], reverse=True)\n","\n","    best_indexes = []\n","    for i in range(len(index_and_score)):\n","        if i >= n_best_size:\n","            break\n","        best_indexes.append(index_and_score[i][0])\n","    return best_indexes\n","\n","\n","def _compute_softmax(scores):\n","    \"\"\"Compute softmax probability over raw logits.\"\"\"\n","    if not scores:\n","        return []\n","\n","    max_score = None\n","    for score in scores:\n","        if max_score is None or score > max_score:\n","            max_score = score\n","\n","    exp_scores = []\n","    total_sum = 0.0\n","    for score in scores:\n","        x = math.exp(score - max_score)\n","        exp_scores.append(x)\n","        total_sum += x\n","\n","    probs = []\n","    for score in exp_scores:\n","        probs.append(score / total_sum)\n","    return probs\n","\n","def warmup_linear(x, warmup=0.002):\n","    if x < warmup:\n","        return x/warmup\n","    return 1.0 - x"],"execution_count":0,"outputs":[]},{"metadata":{"id":"eHX-OZocaDeT","colab_type":"code","outputId":"9406601c-dd76-4940-a5e7-fd1adeb3d114","executionInfo":{"status":"ok","timestamp":1546091949321,"user_tz":-330,"elapsed":5675,"user":{"displayName":"Deep Learning","photoUrl":"","userId":"02647502250279678328"}},"colab":{"base_uri":"https://localhost:8080/","height":268}},"cell_type":"code","source":["model = BertForQuestionAnswering.from_pretrained(\"TrainedModel/\")\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["12/29/2018 13:59:04 - INFO - pytorch_pretrained_bert.modeling -   loading archive file TrainedModel/\n","12/29/2018 13:59:04 - INFO - pytorch_pretrained_bert.modeling -   Model config {\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"max_position_embeddings\": 512,\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"type_vocab_size\": 2,\n","  \"vocab_size\": 30522\n","}\n","\n"],"name":"stderr"}]},{"metadata":{"id":"sj6HsZOGa2Iu","colab_type":"code","outputId":"78512737-d69c-4846-f732-091a79845995","executionInfo":{"status":"ok","timestamp":1546091950070,"user_tz":-330,"elapsed":4494,"user":{"displayName":"Deep Learning","photoUrl":"","userId":"02647502250279678328"}},"colab":{"base_uri":"https://localhost:8080/","height":53}},"cell_type":"code","source":["train_file=\"train-v1.1.json\"\n","predict_file=\"squadtest.json\"\n","do_lower_case=True\n","max_answer_length=30\n","max_answer_length=30\n","output_dir=\".\"\n","n_best_size=20\n","verbose_logging=False\n","bert_model=\"bert-base-uncased\"\n","max_seq_length=128\n","doc_stride=128\n","max_query_length=64\n","local_rank=-1\n","train_batch_size=12\n","predict_batch_size=12\n","num_train_epochs=1\n","gradient_accumulation_steps=1\n","fp16=False\n","warmup_proportion=0.1\n","learning_rate=5e-5\n","tokenizer = BertTokenizer.from_pretrained(bert_model)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["12/29/2018 13:59:09 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"],"name":"stderr"}]},{"metadata":{"id":"ziA0I0-_a_ls","colab_type":"code","colab":{}},"cell_type":"code","source":["cached_train_features_file = train_file+'_{0}_{1}_{2}'.format(bert_model, str(128), str(128))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"bDIbwJ0abEmP","colab_type":"code","colab":{}},"cell_type":"code","source":["train_examples = read_squad_examples(input_file=train_file, is_training=True)\n","num_train_steps = int(len(train_examples) / train_batch_size / gradient_accumulation_steps *num_train_epochs)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"81wshDoLii2K","colab_type":"code","outputId":"00e00533-5105-4354-aa28-3aacada3fc94","executionInfo":{"status":"ok","timestamp":1546091961602,"user_tz":-330,"elapsed":10286,"user":{"displayName":"Deep Learning","photoUrl":"","userId":"02647502250279678328"}},"colab":{"base_uri":"https://localhost:8080/","height":4968}},"cell_type":"code","source":["if local_rank == -1 or args.no_cuda:\n","        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        n_gpu = torch.cuda.device_count()\n","else:\n","    torch.cuda.set_device(args.local_rank)\n","    device = torch.device(\"cuda\", args.local_rank)\n","    n_gpu = 1\n","if fp16:\n","    model.half()\n","model.to(device)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["BertForQuestionAnswering(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(30522, 768)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): BertLayerNorm()\n","      (dropout): Dropout(p=0.1)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): BertLayerNorm()\n","              (dropout): Dropout(p=0.1)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): BertLayerNorm()\n","            (dropout): Dropout(p=0.1)\n","          )\n","        )\n","        (1): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): BertLayerNorm()\n","              (dropout): Dropout(p=0.1)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): BertLayerNorm()\n","            (dropout): Dropout(p=0.1)\n","          )\n","        )\n","        (2): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): BertLayerNorm()\n","              (dropout): Dropout(p=0.1)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): BertLayerNorm()\n","            (dropout): Dropout(p=0.1)\n","          )\n","        )\n","        (3): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): BertLayerNorm()\n","              (dropout): Dropout(p=0.1)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): BertLayerNorm()\n","            (dropout): Dropout(p=0.1)\n","          )\n","        )\n","        (4): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): BertLayerNorm()\n","              (dropout): Dropout(p=0.1)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): BertLayerNorm()\n","            (dropout): Dropout(p=0.1)\n","          )\n","        )\n","        (5): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): BertLayerNorm()\n","              (dropout): Dropout(p=0.1)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): BertLayerNorm()\n","            (dropout): Dropout(p=0.1)\n","          )\n","        )\n","        (6): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): BertLayerNorm()\n","              (dropout): Dropout(p=0.1)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): BertLayerNorm()\n","            (dropout): Dropout(p=0.1)\n","          )\n","        )\n","        (7): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): BertLayerNorm()\n","              (dropout): Dropout(p=0.1)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): BertLayerNorm()\n","            (dropout): Dropout(p=0.1)\n","          )\n","        )\n","        (8): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): BertLayerNorm()\n","              (dropout): Dropout(p=0.1)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): BertLayerNorm()\n","            (dropout): Dropout(p=0.1)\n","          )\n","        )\n","        (9): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): BertLayerNorm()\n","              (dropout): Dropout(p=0.1)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): BertLayerNorm()\n","            (dropout): Dropout(p=0.1)\n","          )\n","        )\n","        (10): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): BertLayerNorm()\n","              (dropout): Dropout(p=0.1)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): BertLayerNorm()\n","            (dropout): Dropout(p=0.1)\n","          )\n","        )\n","        (11): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): BertLayerNorm()\n","              (dropout): Dropout(p=0.1)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): BertLayerNorm()\n","            (dropout): Dropout(p=0.1)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":27}]},{"metadata":{"id":"7B2SrSnLbE0s","colab_type":"code","colab":{}},"cell_type":"code","source":["\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ZkMI5CTabE6a","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"bfLCeM1MBxQD","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"2G83VvxDDuba","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"ZXwza6EdIGqG","colab_type":"code","outputId":"9642faa7-0174-4d7d-cca9-80ca00f8d780","executionInfo":{"status":"ok","timestamp":1546091962894,"user_tz":-330,"elapsed":6198,"user":{"displayName":"Deep Learning","photoUrl":"","userId":"02647502250279678328"}},"colab":{"base_uri":"https://localhost:8080/","height":2161}},"cell_type":"code","source":["eval_examples = read_squad_examples(\n","    input_file=predict_file, is_training=False)\n","eval_features = convert_examples_to_features(\n","    examples=eval_examples,\n","    tokenizer=tokenizer,\n","    max_seq_length=max_seq_length,\n","    doc_stride=doc_stride,\n","    max_query_length=max_query_length,\n","    is_training=False)\n","\n","logger.info(\"***** Running predictions *****\")\n","logger.info(\"  Num orig examples = %d\", len(eval_examples))\n","logger.info(\"  Num split examples = %d\", len(eval_features))\n","logger.info(\"  Batch size = %d\",predict_batch_size)\n","\n","all_input_ids = torch.tensor([f.input_ids for f in eval_features], dtype=torch.long)\n","all_input_mask = torch.tensor([f.input_mask for f in eval_features], dtype=torch.long)\n","all_segment_ids = torch.tensor([f.segment_ids for f in eval_features], dtype=torch.long)\n","all_example_index = torch.arange(all_input_ids.size(0), dtype=torch.long)\n","eval_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_example_index)\n","# Run prediction for full data\n","eval_sampler = SequentialSampler(eval_data)\n","eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=predict_batch_size)\n","\n","model.eval()\n","all_results = []\n","logger.info(\"Start evaluating\")\n","for input_ids, input_mask, segment_ids, example_indices in tqdm(eval_dataloader, desc=\"Evaluating\"):\n","    if len(all_results) % 1000 == 0:\n","        logger.info(\"Processing example: %d\" % (len(all_results)))\n","    input_ids = input_ids.to(device)\n","    input_mask = input_mask.to(device)\n","    segment_ids = segment_ids.to(device)\n","    with torch.no_grad():\n","        batch_start_logits, batch_end_logits = model(input_ids, segment_ids, input_mask)\n","    for i, example_index in enumerate(example_indices):\n","        start_logits = batch_start_logits[i].detach().cpu().tolist()\n","        end_logits = batch_end_logits[i].detach().cpu().tolist()\n","        eval_feature = eval_features[example_index.item()]\n","        unique_id = int(eval_feature.unique_id)\n","        all_results.append(RawResult(unique_id=unique_id,\n","                                     start_logits=start_logits,\n","                                     end_logits=end_logits))\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["12/29/2018 13:59:21 - INFO - __main__ -   *** Example ***\n","12/29/2018 13:59:21 - INFO - __main__ -   unique_id: 1000000000\n","12/29/2018 13:59:21 - INFO - __main__ -   example_index: 0\n","12/29/2018 13:59:21 - INFO - __main__ -   doc_span_index: 0\n","12/29/2018 13:59:21 - INFO - __main__ -   tokens: [CLS] what rainforest covers the majority of the amazon basin in south america ? [SEP] the amazon rainforest ( portuguese : flores ##ta amazon ##ica or amazon ##ia ; spanish : se ##lva amazon ##ica , amazon ##ia or usually amazon ##ia ; french : fore ##t amazon ##ien ##ne ; dutch : amazon ##ere ##gen ##wo ##ud ) , also known in english as amazon ##ia or the amazon jungle , is a moist broad ##leaf forest that covers most of the amazon basin of south america . this basin encompasses 7 , 000 , 000 square kilometres ( 2 , 700 , 000 sq mi ) , of which 5 , 500 , 000 square kilometres ( 2 , 100 , 000 sq mi ) [SEP]\n","12/29/2018 13:59:21 - INFO - __main__ -   token_to_orig_map: 15:0 16:1 17:2 18:3 19:3 20:3 21:4 22:4 23:5 24:5 25:6 26:7 27:7 28:7 29:8 30:8 31:9 32:9 33:10 34:10 35:10 36:11 37:11 38:12 39:13 40:14 41:14 42:14 43:15 44:15 45:16 46:16 47:17 48:17 49:17 50:17 51:18 52:18 53:19 54:19 55:19 56:19 57:19 58:19 59:19 60:20 61:21 62:22 63:23 64:24 65:25 66:25 67:26 68:27 69:28 70:29 71:29 72:30 73:31 74:32 75:33 76:33 77:34 78:35 79:36 80:37 81:38 82:39 83:40 84:41 85:42 86:43 87:44 88:44 89:45 90:46 91:47 92:48 93:48 94:48 95:48 96:48 97:49 98:50 99:51 100:51 101:51 102:51 103:51 104:51 105:52 106:53 107:53 108:53 109:54 110:55 111:56 112:56 113:56 114:56 115:56 116:57 117:58 118:59 119:59 120:59 121:59 122:59 123:59 124:60 125:61 126:61\n","12/29/2018 13:59:21 - INFO - __main__ -   token_is_max_context: 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True\n","12/29/2018 13:59:21 - INFO - __main__ -   input_ids: 101 2054 18951 4472 1996 3484 1997 1996 9733 6403 1999 2148 2637 1029 102 1996 9733 18951 1006 5077 1024 17343 2696 9733 5555 2030 9733 2401 1025 3009 1024 7367 22144 9733 5555 1010 9733 2401 2030 2788 9733 2401 1025 2413 1024 18921 2102 9733 9013 2638 1025 3803 1024 9733 7869 6914 12155 6784 1007 1010 2036 2124 1999 2394 2004 9733 2401 2030 1996 9733 8894 1010 2003 1037 11052 5041 19213 3224 2008 4472 2087 1997 1996 9733 6403 1997 2148 2637 1012 2023 6403 13974 1021 1010 2199 1010 2199 2675 3717 1006 1016 1010 6352 1010 2199 5490 2771 1007 1010 1997 2029 1019 1010 3156 1010 2199 2675 3717 1006 1016 1010 2531 1010 2199 5490 2771 1007 102\n","12/29/2018 13:59:21 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n","12/29/2018 13:59:21 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n","12/29/2018 13:59:21 - INFO - __main__ -   *** Example ***\n","12/29/2018 13:59:21 - INFO - __main__ -   unique_id: 1000000001\n","12/29/2018 13:59:21 - INFO - __main__ -   example_index: 0\n","12/29/2018 13:59:21 - INFO - __main__ -   doc_span_index: 1\n","12/29/2018 13:59:21 - INFO - __main__ -   tokens: [CLS] what rainforest covers the majority of the amazon basin in south america ? [SEP] are covered by the rainforest . this region includes territory belonging to nine nations . the majority of the forest is contained within brazil , with 60 % of the rainforest , followed by peru with 13 % , colombia with 10 % , and with minor amounts in venezuela , ecuador , bolivia , guyana , suriname and french guiana . states or departments in four nations contain \" amazon ##as \" in their names . the amazon represents over half of the planet ' s remaining rainforest ##s , and comprises the largest and most bio ##di ##verse tract of tropical rainforest in the world , with an estimated 390 [SEP]\n","12/29/2018 13:59:21 - INFO - __main__ -   token_to_orig_map: 15:62 16:63 17:64 18:65 19:66 20:66 21:67 22:68 23:69 24:70 25:71 26:72 27:73 28:74 29:74 30:75 31:76 32:77 33:78 34:79 35:80 36:81 37:82 38:83 39:83 40:84 41:85 42:85 43:86 44:87 45:88 46:88 47:89 48:90 49:91 50:92 51:93 52:93 53:93 54:94 55:95 56:96 57:96 58:96 59:97 60:98 61:99 62:100 63:101 64:102 65:102 66:103 67:103 68:104 69:104 70:105 71:105 72:106 73:107 74:108 75:109 76:109 77:110 78:111 79:112 80:113 81:114 82:115 83:116 84:117 85:117 86:117 87:117 88:118 89:119 90:120 91:120 92:121 93:122 94:123 95:124 96:125 97:126 98:127 99:128 100:128 101:128 102:129 103:130 104:130 105:130 106:131 107:132 108:133 109:134 110:135 111:136 112:137 113:137 114:137 115:138 116:139 117:140 118:141 119:142 120:143 121:144 122:144 123:145 124:146 125:147 126:148\n","12/29/2018 13:59:21 - INFO - __main__ -   token_is_max_context: 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True\n","12/29/2018 13:59:21 - INFO - __main__ -   input_ids: 101 2054 18951 4472 1996 3484 1997 1996 9733 6403 1999 2148 2637 1029 102 2024 3139 2011 1996 18951 1012 2023 2555 2950 3700 7495 2000 3157 3741 1012 1996 3484 1997 1996 3224 2003 4838 2306 4380 1010 2007 3438 1003 1997 1996 18951 1010 2628 2011 7304 2007 2410 1003 1010 7379 2007 2184 1003 1010 1998 2007 3576 8310 1999 8326 1010 10378 1010 11645 1010 18786 1010 25050 1998 2413 23568 1012 2163 2030 7640 1999 2176 3741 5383 1000 9733 3022 1000 1999 2037 3415 1012 1996 9733 5836 2058 2431 1997 1996 4774 1005 1055 3588 18951 2015 1010 1998 8681 1996 2922 1998 2087 16012 4305 16070 12859 1997 5133 18951 1999 1996 2088 1010 2007 2019 4358 20024 102\n","12/29/2018 13:59:21 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n","12/29/2018 13:59:21 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n","12/29/2018 13:59:21 - INFO - __main__ -   *** Example ***\n","12/29/2018 13:59:21 - INFO - __main__ -   unique_id: 1000000002\n","12/29/2018 13:59:21 - INFO - __main__ -   example_index: 0\n","12/29/2018 13:59:21 - INFO - __main__ -   doc_span_index: 2\n","12/29/2018 13:59:21 - INFO - __main__ -   tokens: [CLS] what rainforest covers the majority of the amazon basin in south america ? [SEP] billion individual trees divided into 16 , 000 species . [SEP]\n","12/29/2018 13:59:21 - INFO - __main__ -   token_to_orig_map: 15:149 16:150 17:151 18:152 19:153 20:154 21:154 22:154 23:155 24:155\n","12/29/2018 13:59:21 - INFO - __main__ -   token_is_max_context: 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True\n","12/29/2018 13:59:21 - INFO - __main__ -   input_ids: 101 2054 18951 4472 1996 3484 1997 1996 9733 6403 1999 2148 2637 1029 102 4551 3265 3628 4055 2046 2385 1010 2199 2427 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","12/29/2018 13:59:21 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","12/29/2018 13:59:21 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","12/29/2018 13:59:21 - INFO - __main__ -   *** Example ***\n","12/29/2018 13:59:21 - INFO - __main__ -   unique_id: 1000000003\n","12/29/2018 13:59:21 - INFO - __main__ -   example_index: 1\n","12/29/2018 13:59:21 - INFO - __main__ -   doc_span_index: 0\n","12/29/2018 13:59:21 - INFO - __main__ -   tokens: [CLS] the amazon rainforest makes up what amount of earth ' s rainforest ##s ? [SEP] the amazon rainforest ( portuguese : flores ##ta amazon ##ica or amazon ##ia ; spanish : se ##lva amazon ##ica , amazon ##ia or usually amazon ##ia ; french : fore ##t amazon ##ien ##ne ; dutch : amazon ##ere ##gen ##wo ##ud ) , also known in english as amazon ##ia or the amazon jungle , is a moist broad ##leaf forest that covers most of the amazon basin of south america . this basin encompasses 7 , 000 , 000 square kilometres ( 2 , 700 , 000 sq mi ) , of which 5 , 500 , 000 square kilometres ( 2 , 100 , 000 sq mi [SEP]\n","12/29/2018 13:59:21 - INFO - __main__ -   token_to_orig_map: 16:0 17:1 18:2 19:3 20:3 21:3 22:4 23:4 24:5 25:5 26:6 27:7 28:7 29:7 30:8 31:8 32:9 33:9 34:10 35:10 36:10 37:11 38:11 39:12 40:13 41:14 42:14 43:14 44:15 45:15 46:16 47:16 48:17 49:17 50:17 51:17 52:18 53:18 54:19 55:19 56:19 57:19 58:19 59:19 60:19 61:20 62:21 63:22 64:23 65:24 66:25 67:25 68:26 69:27 70:28 71:29 72:29 73:30 74:31 75:32 76:33 77:33 78:34 79:35 80:36 81:37 82:38 83:39 84:40 85:41 86:42 87:43 88:44 89:44 90:45 91:46 92:47 93:48 94:48 95:48 96:48 97:48 98:49 99:50 100:51 101:51 102:51 103:51 104:51 105:51 106:52 107:53 108:53 109:53 110:54 111:55 112:56 113:56 114:56 115:56 116:56 117:57 118:58 119:59 120:59 121:59 122:59 123:59 124:59 125:60 126:61\n","12/29/2018 13:59:21 - INFO - __main__ -   token_is_max_context: 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True\n","12/29/2018 13:59:21 - INFO - __main__ -   input_ids: 101 1996 9733 18951 3084 2039 2054 3815 1997 3011 1005 1055 18951 2015 1029 102 1996 9733 18951 1006 5077 1024 17343 2696 9733 5555 2030 9733 2401 1025 3009 1024 7367 22144 9733 5555 1010 9733 2401 2030 2788 9733 2401 1025 2413 1024 18921 2102 9733 9013 2638 1025 3803 1024 9733 7869 6914 12155 6784 1007 1010 2036 2124 1999 2394 2004 9733 2401 2030 1996 9733 8894 1010 2003 1037 11052 5041 19213 3224 2008 4472 2087 1997 1996 9733 6403 1997 2148 2637 1012 2023 6403 13974 1021 1010 2199 1010 2199 2675 3717 1006 1016 1010 6352 1010 2199 5490 2771 1007 1010 1997 2029 1019 1010 3156 1010 2199 2675 3717 1006 1016 1010 2531 1010 2199 5490 2771 102\n","12/29/2018 13:59:21 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n","12/29/2018 13:59:21 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n","12/29/2018 13:59:21 - INFO - __main__ -   *** Example ***\n","12/29/2018 13:59:21 - INFO - __main__ -   unique_id: 1000000004\n","12/29/2018 13:59:21 - INFO - __main__ -   example_index: 1\n","12/29/2018 13:59:21 - INFO - __main__ -   doc_span_index: 1\n","12/29/2018 13:59:21 - INFO - __main__ -   tokens: [CLS] the amazon rainforest makes up what amount of earth ' s rainforest ##s ? [SEP] ) are covered by the rainforest . this region includes territory belonging to nine nations . the majority of the forest is contained within brazil , with 60 % of the rainforest , followed by peru with 13 % , colombia with 10 % , and with minor amounts in venezuela , ecuador , bolivia , guyana , suriname and french guiana . states or departments in four nations contain \" amazon ##as \" in their names . the amazon represents over half of the planet ' s remaining rainforest ##s , and comprises the largest and most bio ##di ##verse tract of tropical rainforest in the world , with an [SEP]\n","12/29/2018 13:59:21 - INFO - __main__ -   token_to_orig_map: 16:61 17:62 18:63 19:64 20:65 21:66 22:66 23:67 24:68 25:69 26:70 27:71 28:72 29:73 30:74 31:74 32:75 33:76 34:77 35:78 36:79 37:80 38:81 39:82 40:83 41:83 42:84 43:85 44:85 45:86 46:87 47:88 48:88 49:89 50:90 51:91 52:92 53:93 54:93 55:93 56:94 57:95 58:96 59:96 60:96 61:97 62:98 63:99 64:100 65:101 66:102 67:102 68:103 69:103 70:104 71:104 72:105 73:105 74:106 75:107 76:108 77:109 78:109 79:110 80:111 81:112 82:113 83:114 84:115 85:116 86:117 87:117 88:117 89:117 90:118 91:119 92:120 93:120 94:121 95:122 96:123 97:124 98:125 99:126 100:127 101:128 102:128 103:128 104:129 105:130 106:130 107:130 108:131 109:132 110:133 111:134 112:135 113:136 114:137 115:137 116:137 117:138 118:139 119:140 120:141 121:142 122:143 123:144 124:144 125:145 126:146\n","12/29/2018 13:59:21 - INFO - __main__ -   token_is_max_context: 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True\n","12/29/2018 13:59:21 - INFO - __main__ -   input_ids: 101 1996 9733 18951 3084 2039 2054 3815 1997 3011 1005 1055 18951 2015 1029 102 1007 2024 3139 2011 1996 18951 1012 2023 2555 2950 3700 7495 2000 3157 3741 1012 1996 3484 1997 1996 3224 2003 4838 2306 4380 1010 2007 3438 1003 1997 1996 18951 1010 2628 2011 7304 2007 2410 1003 1010 7379 2007 2184 1003 1010 1998 2007 3576 8310 1999 8326 1010 10378 1010 11645 1010 18786 1010 25050 1998 2413 23568 1012 2163 2030 7640 1999 2176 3741 5383 1000 9733 3022 1000 1999 2037 3415 1012 1996 9733 5836 2058 2431 1997 1996 4774 1005 1055 3588 18951 2015 1010 1998 8681 1996 2922 1998 2087 16012 4305 16070 12859 1997 5133 18951 1999 1996 2088 1010 2007 2019 102\n","12/29/2018 13:59:21 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n","12/29/2018 13:59:21 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n","12/29/2018 13:59:21 - INFO - __main__ -   *** Example ***\n","12/29/2018 13:59:21 - INFO - __main__ -   unique_id: 1000000005\n","12/29/2018 13:59:21 - INFO - __main__ -   example_index: 1\n","12/29/2018 13:59:21 - INFO - __main__ -   doc_span_index: 2\n","12/29/2018 13:59:21 - INFO - __main__ -   tokens: [CLS] the amazon rainforest makes up what amount of earth ' s rainforest ##s ? [SEP] estimated 390 billion individual trees divided into 16 , 000 species . [SEP]\n","12/29/2018 13:59:21 - INFO - __main__ -   token_to_orig_map: 16:147 17:148 18:149 19:150 20:151 21:152 22:153 23:154 24:154 25:154 26:155 27:155\n","12/29/2018 13:59:21 - INFO - __main__ -   token_is_max_context: 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True\n","12/29/2018 13:59:21 - INFO - __main__ -   input_ids: 101 1996 9733 18951 3084 2039 2054 3815 1997 3011 1005 1055 18951 2015 1029 102 4358 20024 4551 3265 3628 4055 2046 2385 1010 2199 2427 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","12/29/2018 13:59:21 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","12/29/2018 13:59:21 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","12/29/2018 13:59:21 - INFO - __main__ -   *** Example ***\n","12/29/2018 13:59:21 - INFO - __main__ -   unique_id: 1000000006\n","12/29/2018 13:59:21 - INFO - __main__ -   example_index: 2\n","12/29/2018 13:59:21 - INFO - __main__ -   doc_span_index: 0\n","12/29/2018 13:59:21 - INFO - __main__ -   tokens: [CLS] how many square kilometers is the amazon basin ? [SEP] the amazon rainforest ( portuguese : flores ##ta amazon ##ica or amazon ##ia ; spanish : se ##lva amazon ##ica , amazon ##ia or usually amazon ##ia ; french : fore ##t amazon ##ien ##ne ; dutch : amazon ##ere ##gen ##wo ##ud ) , also known in english as amazon ##ia or the amazon jungle , is a moist broad ##leaf forest that covers most of the amazon basin of south america . this basin encompasses 7 , 000 , 000 square kilometres ( 2 , 700 , 000 sq mi ) , of which 5 , 500 , 000 square kilometres ( 2 , 100 , 000 sq mi ) are covered by the [SEP]\n","12/29/2018 13:59:21 - INFO - __main__ -   token_to_orig_map: 11:0 12:1 13:2 14:3 15:3 16:3 17:4 18:4 19:5 20:5 21:6 22:7 23:7 24:7 25:8 26:8 27:9 28:9 29:10 30:10 31:10 32:11 33:11 34:12 35:13 36:14 37:14 38:14 39:15 40:15 41:16 42:16 43:17 44:17 45:17 46:17 47:18 48:18 49:19 50:19 51:19 52:19 53:19 54:19 55:19 56:20 57:21 58:22 59:23 60:24 61:25 62:25 63:26 64:27 65:28 66:29 67:29 68:30 69:31 70:32 71:33 72:33 73:34 74:35 75:36 76:37 77:38 78:39 79:40 80:41 81:42 82:43 83:44 84:44 85:45 86:46 87:47 88:48 89:48 90:48 91:48 92:48 93:49 94:50 95:51 96:51 97:51 98:51 99:51 100:51 101:52 102:53 103:53 104:53 105:54 106:55 107:56 108:56 109:56 110:56 111:56 112:57 113:58 114:59 115:59 116:59 117:59 118:59 119:59 120:60 121:61 122:61 123:62 124:63 125:64 126:65\n","12/29/2018 13:59:21 - INFO - __main__ -   token_is_max_context: 11:True 12:True 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True\n","12/29/2018 13:59:21 - INFO - __main__ -   input_ids: 101 2129 2116 2675 7338 2003 1996 9733 6403 1029 102 1996 9733 18951 1006 5077 1024 17343 2696 9733 5555 2030 9733 2401 1025 3009 1024 7367 22144 9733 5555 1010 9733 2401 2030 2788 9733 2401 1025 2413 1024 18921 2102 9733 9013 2638 1025 3803 1024 9733 7869 6914 12155 6784 1007 1010 2036 2124 1999 2394 2004 9733 2401 2030 1996 9733 8894 1010 2003 1037 11052 5041 19213 3224 2008 4472 2087 1997 1996 9733 6403 1997 2148 2637 1012 2023 6403 13974 1021 1010 2199 1010 2199 2675 3717 1006 1016 1010 6352 1010 2199 5490 2771 1007 1010 1997 2029 1019 1010 3156 1010 2199 2675 3717 1006 1016 1010 2531 1010 2199 5490 2771 1007 2024 3139 2011 1996 102\n","12/29/2018 13:59:21 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n","12/29/2018 13:59:21 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n","12/29/2018 13:59:21 - INFO - __main__ -   *** Example ***\n","12/29/2018 13:59:21 - INFO - __main__ -   unique_id: 1000000007\n","12/29/2018 13:59:21 - INFO - __main__ -   example_index: 2\n","12/29/2018 13:59:21 - INFO - __main__ -   doc_span_index: 1\n","12/29/2018 13:59:21 - INFO - __main__ -   tokens: [CLS] how many square kilometers is the amazon basin ? [SEP] rainforest . this region includes territory belonging to nine nations . the majority of the forest is contained within brazil , with 60 % of the rainforest , followed by peru with 13 % , colombia with 10 % , and with minor amounts in venezuela , ecuador , bolivia , guyana , suriname and french guiana . states or departments in four nations contain \" amazon ##as \" in their names . the amazon represents over half of the planet ' s remaining rainforest ##s , and comprises the largest and most bio ##di ##verse tract of tropical rainforest in the world , with an estimated 390 billion individual trees divided into 16 , 000 [SEP]\n","12/29/2018 13:59:21 - INFO - __main__ -   token_to_orig_map: 11:66 12:66 13:67 14:68 15:69 16:70 17:71 18:72 19:73 20:74 21:74 22:75 23:76 24:77 25:78 26:79 27:80 28:81 29:82 30:83 31:83 32:84 33:85 34:85 35:86 36:87 37:88 38:88 39:89 40:90 41:91 42:92 43:93 44:93 45:93 46:94 47:95 48:96 49:96 50:96 51:97 52:98 53:99 54:100 55:101 56:102 57:102 58:103 59:103 60:104 61:104 62:105 63:105 64:106 65:107 66:108 67:109 68:109 69:110 70:111 71:112 72:113 73:114 74:115 75:116 76:117 77:117 78:117 79:117 80:118 81:119 82:120 83:120 84:121 85:122 86:123 87:124 88:125 89:126 90:127 91:128 92:128 93:128 94:129 95:130 96:130 97:130 98:131 99:132 100:133 101:134 102:135 103:136 104:137 105:137 106:137 107:138 108:139 109:140 110:141 111:142 112:143 113:144 114:144 115:145 116:146 117:147 118:148 119:149 120:150 121:151 122:152 123:153 124:154 125:154 126:154\n","12/29/2018 13:59:21 - INFO - __main__ -   token_is_max_context: 11:True 12:True 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True\n","12/29/2018 13:59:21 - INFO - __main__ -   input_ids: 101 2129 2116 2675 7338 2003 1996 9733 6403 1029 102 18951 1012 2023 2555 2950 3700 7495 2000 3157 3741 1012 1996 3484 1997 1996 3224 2003 4838 2306 4380 1010 2007 3438 1003 1997 1996 18951 1010 2628 2011 7304 2007 2410 1003 1010 7379 2007 2184 1003 1010 1998 2007 3576 8310 1999 8326 1010 10378 1010 11645 1010 18786 1010 25050 1998 2413 23568 1012 2163 2030 7640 1999 2176 3741 5383 1000 9733 3022 1000 1999 2037 3415 1012 1996 9733 5836 2058 2431 1997 1996 4774 1005 1055 3588 18951 2015 1010 1998 8681 1996 2922 1998 2087 16012 4305 16070 12859 1997 5133 18951 1999 1996 2088 1010 2007 2019 4358 20024 4551 3265 3628 4055 2046 2385 1010 2199 102\n","12/29/2018 13:59:21 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n","12/29/2018 13:59:21 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n","12/29/2018 13:59:21 - INFO - __main__ -   *** Example ***\n","12/29/2018 13:59:21 - INFO - __main__ -   unique_id: 1000000008\n","12/29/2018 13:59:21 - INFO - __main__ -   example_index: 2\n","12/29/2018 13:59:21 - INFO - __main__ -   doc_span_index: 2\n","12/29/2018 13:59:21 - INFO - __main__ -   tokens: [CLS] how many square kilometers is the amazon basin ? [SEP] species . [SEP]\n","12/29/2018 13:59:21 - INFO - __main__ -   token_to_orig_map: 11:155 12:155\n","12/29/2018 13:59:21 - INFO - __main__ -   token_is_max_context: 11:True 12:True\n","12/29/2018 13:59:21 - INFO - __main__ -   input_ids: 101 2129 2116 2675 7338 2003 1996 9733 6403 1029 102 2427 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","12/29/2018 13:59:21 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","12/29/2018 13:59:21 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","12/29/2018 13:59:21 - INFO - __main__ -   *** Example ***\n","12/29/2018 13:59:21 - INFO - __main__ -   unique_id: 1000000009\n","12/29/2018 13:59:21 - INFO - __main__ -   example_index: 3\n","12/29/2018 13:59:21 - INFO - __main__ -   doc_span_index: 0\n","12/29/2018 13:59:21 - INFO - __main__ -   tokens: [CLS] what is the estimate for the amount of tree species in the amazon tropical rain forest ? [SEP] the amazon rainforest ( portuguese : flores ##ta amazon ##ica or amazon ##ia ; spanish : se ##lva amazon ##ica , amazon ##ia or usually amazon ##ia ; french : fore ##t amazon ##ien ##ne ; dutch : amazon ##ere ##gen ##wo ##ud ) , also known in english as amazon ##ia or the amazon jungle , is a moist broad ##leaf forest that covers most of the amazon basin of south america . this basin encompasses 7 , 000 , 000 square kilometres ( 2 , 700 , 000 sq mi ) , of which 5 , 500 , 000 square kilometres ( 2 , 100 , [SEP]\n","12/29/2018 13:59:21 - INFO - __main__ -   token_to_orig_map: 19:0 20:1 21:2 22:3 23:3 24:3 25:4 26:4 27:5 28:5 29:6 30:7 31:7 32:7 33:8 34:8 35:9 36:9 37:10 38:10 39:10 40:11 41:11 42:12 43:13 44:14 45:14 46:14 47:15 48:15 49:16 50:16 51:17 52:17 53:17 54:17 55:18 56:18 57:19 58:19 59:19 60:19 61:19 62:19 63:19 64:20 65:21 66:22 67:23 68:24 69:25 70:25 71:26 72:27 73:28 74:29 75:29 76:30 77:31 78:32 79:33 80:33 81:34 82:35 83:36 84:37 85:38 86:39 87:40 88:41 89:42 90:43 91:44 92:44 93:45 94:46 95:47 96:48 97:48 98:48 99:48 100:48 101:49 102:50 103:51 104:51 105:51 106:51 107:51 108:51 109:52 110:53 111:53 112:53 113:54 114:55 115:56 116:56 117:56 118:56 119:56 120:57 121:58 122:59 123:59 124:59 125:59 126:59\n","12/29/2018 13:59:21 - INFO - __main__ -   token_is_max_context: 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True\n","12/29/2018 13:59:21 - INFO - __main__ -   input_ids: 101 2054 2003 1996 10197 2005 1996 3815 1997 3392 2427 1999 1996 9733 5133 4542 3224 1029 102 1996 9733 18951 1006 5077 1024 17343 2696 9733 5555 2030 9733 2401 1025 3009 1024 7367 22144 9733 5555 1010 9733 2401 2030 2788 9733 2401 1025 2413 1024 18921 2102 9733 9013 2638 1025 3803 1024 9733 7869 6914 12155 6784 1007 1010 2036 2124 1999 2394 2004 9733 2401 2030 1996 9733 8894 1010 2003 1037 11052 5041 19213 3224 2008 4472 2087 1997 1996 9733 6403 1997 2148 2637 1012 2023 6403 13974 1021 1010 2199 1010 2199 2675 3717 1006 1016 1010 6352 1010 2199 5490 2771 1007 1010 1997 2029 1019 1010 3156 1010 2199 2675 3717 1006 1016 1010 2531 1010 102\n","12/29/2018 13:59:21 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n","12/29/2018 13:59:21 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n","12/29/2018 13:59:21 - INFO - __main__ -   *** Example ***\n","12/29/2018 13:59:21 - INFO - __main__ -   unique_id: 1000000010\n","12/29/2018 13:59:21 - INFO - __main__ -   example_index: 3\n","12/29/2018 13:59:21 - INFO - __main__ -   doc_span_index: 1\n","12/29/2018 13:59:21 - INFO - __main__ -   tokens: [CLS] what is the estimate for the amount of tree species in the amazon tropical rain forest ? [SEP] 000 sq mi ) are covered by the rainforest . this region includes territory belonging to nine nations . the majority of the forest is contained within brazil , with 60 % of the rainforest , followed by peru with 13 % , colombia with 10 % , and with minor amounts in venezuela , ecuador , bolivia , guyana , suriname and french guiana . states or departments in four nations contain \" amazon ##as \" in their names . the amazon represents over half of the planet ' s remaining rainforest ##s , and comprises the largest and most bio ##di ##verse tract of tropical rainforest [SEP]\n","12/29/2018 13:59:21 - INFO - __main__ -   token_to_orig_map: 19:59 20:60 21:61 22:61 23:62 24:63 25:64 26:65 27:66 28:66 29:67 30:68 31:69 32:70 33:71 34:72 35:73 36:74 37:74 38:75 39:76 40:77 41:78 42:79 43:80 44:81 45:82 46:83 47:83 48:84 49:85 50:85 51:86 52:87 53:88 54:88 55:89 56:90 57:91 58:92 59:93 60:93 61:93 62:94 63:95 64:96 65:96 66:96 67:97 68:98 69:99 70:100 71:101 72:102 73:102 74:103 75:103 76:104 77:104 78:105 79:105 80:106 81:107 82:108 83:109 84:109 85:110 86:111 87:112 88:113 89:114 90:115 91:116 92:117 93:117 94:117 95:117 96:118 97:119 98:120 99:120 100:121 101:122 102:123 103:124 104:125 105:126 106:127 107:128 108:128 109:128 110:129 111:130 112:130 113:130 114:131 115:132 116:133 117:134 118:135 119:136 120:137 121:137 122:137 123:138 124:139 125:140 126:141\n","12/29/2018 13:59:21 - INFO - __main__ -   token_is_max_context: 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True\n","12/29/2018 13:59:21 - INFO - __main__ -   input_ids: 101 2054 2003 1996 10197 2005 1996 3815 1997 3392 2427 1999 1996 9733 5133 4542 3224 1029 102 2199 5490 2771 1007 2024 3139 2011 1996 18951 1012 2023 2555 2950 3700 7495 2000 3157 3741 1012 1996 3484 1997 1996 3224 2003 4838 2306 4380 1010 2007 3438 1003 1997 1996 18951 1010 2628 2011 7304 2007 2410 1003 1010 7379 2007 2184 1003 1010 1998 2007 3576 8310 1999 8326 1010 10378 1010 11645 1010 18786 1010 25050 1998 2413 23568 1012 2163 2030 7640 1999 2176 3741 5383 1000 9733 3022 1000 1999 2037 3415 1012 1996 9733 5836 2058 2431 1997 1996 4774 1005 1055 3588 18951 2015 1010 1998 8681 1996 2922 1998 2087 16012 4305 16070 12859 1997 5133 18951 102\n","12/29/2018 13:59:21 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n","12/29/2018 13:59:21 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n","12/29/2018 13:59:21 - INFO - __main__ -   *** Example ***\n","12/29/2018 13:59:21 - INFO - __main__ -   unique_id: 1000000011\n","12/29/2018 13:59:21 - INFO - __main__ -   example_index: 3\n","12/29/2018 13:59:21 - INFO - __main__ -   doc_span_index: 2\n","12/29/2018 13:59:21 - INFO - __main__ -   tokens: [CLS] what is the estimate for the amount of tree species in the amazon tropical rain forest ? [SEP] in the world , with an estimated 390 billion individual trees divided into 16 , 000 species . [SEP]\n","12/29/2018 13:59:21 - INFO - __main__ -   token_to_orig_map: 19:142 20:143 21:144 22:144 23:145 24:146 25:147 26:148 27:149 28:150 29:151 30:152 31:153 32:154 33:154 34:154 35:155 36:155\n","12/29/2018 13:59:21 - INFO - __main__ -   token_is_max_context: 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True\n","12/29/2018 13:59:21 - INFO - __main__ -   input_ids: 101 2054 2003 1996 10197 2005 1996 3815 1997 3392 2427 1999 1996 9733 5133 4542 3224 1029 102 1999 1996 2088 1010 2007 2019 4358 20024 4551 3265 3628 4055 2046 2385 1010 2199 2427 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","12/29/2018 13:59:21 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","12/29/2018 13:59:21 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","12/29/2018 13:59:21 - INFO - __main__ -   ***** Running predictions *****\n","12/29/2018 13:59:21 - INFO - __main__ -     Num orig examples = 4\n","12/29/2018 13:59:21 - INFO - __main__ -     Num split examples = 12\n","12/29/2018 13:59:21 - INFO - __main__ -     Batch size = 12\n","12/29/2018 13:59:22 - INFO - __main__ -   Start evaluating\n","Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]12/29/2018 13:59:22 - INFO - __main__ -   Processing example: 0\n","Evaluating: 100%|██████████| 1/1 [00:00<00:00,  3.58it/s]\n"],"name":"stderr"}]},{"metadata":{"id":"Dq4n_E_UNumk","colab_type":"code","outputId":"8591c463-0e26-446e-b885-703a0e2d0b57","executionInfo":{"status":"ok","timestamp":1546092027500,"user_tz":-330,"elapsed":1437,"user":{"displayName":"Deep Learning","photoUrl":"","userId":"02647502250279678328"}},"colab":{"base_uri":"https://localhost:8080/","height":50}},"cell_type":"code","source":["output_prediction_file = os.path.join(output_dir, \"squadtestpredictions.json\")\n","output_nbest_file = os.path.join(output_dir, \"nbest_squadtestpredictions.json\")\n","write_predictions(eval_examples, eval_features, all_results,\n","                  n_best_size, max_answer_length,\n","                  do_lower_case, output_prediction_file,\n","                  output_nbest_file,verbose_logging)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["12/29/2018 14:00:26 - INFO - __main__ -   Writing predictions to: ./squadtestpredictions.json\n","12/29/2018 14:00:26 - INFO - __main__ -   Writing nbest to: ./nbest_squadtestpredictions.json\n"],"name":"stderr"}]},{"metadata":{"id":"J-UPv41Ointg","colab_type":"code","outputId":"7f986f0c-6f2d-4ab3-e4d9-f32d9215138f","executionInfo":{"status":"ok","timestamp":1546092033948,"user_tz":-330,"elapsed":3600,"user":{"displayName":"Deep Learning","photoUrl":"","userId":"02647502250279678328"}},"colab":{"base_uri":"https://localhost:8080/","height":167}},"cell_type":"code","source":["ls"],"execution_count":0,"outputs":[{"output_type":"stream","text":[" \u001b[0m\u001b[01;34mclouderizer\u001b[0m/                      pytorch_modeltrained_2_epochs.bin\n","\u001b[01;34m'Colab Notebooks'\u001b[0m/                 squadtest.json\n"," dev-v1.1.json                     squadtestpredictions.json\n"," nbest_predictions_2_epochs.json   testpytorch_model.bin\n"," nbest_predictions.json            testpytorch_modeltrained.bin\n"," nbest_squadtestpredictions.json   \u001b[01;34mTrainedModel\u001b[0m/\n"," predictions_2_epochs.json         train-v1.1.json\n"," predictions.json                  train-v1.1.json_bert-base-uncased_128_128\n"," pytorch_model.bin\n"],"name":"stdout"}]},{"metadata":{"id":"7YQl0NM8B3NO","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"3ognfVIkh_Cp","colab_type":"text"},"cell_type":"markdown","source":["# New Section"]},{"metadata":{"id":"1BlZlvRWbFbH","colab_type":"code","outputId":"659ebd5e-ccec-4b01-e63f-e2adab930f05","executionInfo":{"status":"ok","timestamp":1545910480675,"user_tz":-330,"elapsed":7077,"user":{"displayName":"Deep Learning","photoUrl":"","userId":"02647502250279678328"}},"colab":{"base_uri":"https://localhost:8080/","height":84}},"cell_type":"code","source":["!ls"],"execution_count":0,"outputs":[{"output_type":"stream","text":[" clouderizer\t     testpytorch_model.bin\n","'Colab Notebooks'    testpytorch_modeltrained.bin\n"," dev-v1.1.json\t     train-v1.1.json\n"," pytorch_model.bin   train-v1.1.json_bert-base-uncased_128_128\n"],"name":"stdout"}]},{"metadata":{"id":"mf8kO6OSbFY0","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"k1R_WfQcbFTd","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"H46xcqImbFQ0","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"5GVpp0_abFOX","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"W1U0xru8bFIm","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"-5E2pLYEbFDL","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"zck-I3bDbFAl","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"LrlraFiTbE5S","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"61eAfNTobEyT","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"MDb8KSmAbCv_","colab_type":"text"},"cell_type":"markdown","source":[""]}]}